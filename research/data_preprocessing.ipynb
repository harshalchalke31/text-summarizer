{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\python\\\\text-summarizer\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\python\\\\text-summarizer'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_root = '.\\data\\\\ami'\n",
    "dialogsum_root = '.\\\\data\\\\dialogsum'\n",
    "samsum_root = '.\\\\data\\\\samsum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save paths\n",
    "save_directory = \".\\\\data\\\\summary_data\"\n",
    "os.makedirs(save_directory,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AMI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 209 entries, 0 to 208\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        209 non-null    int64 \n",
      " 1   dialogue  209 non-null    object\n",
      " 2   summary   209 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28 entries, 0 to 27\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        28 non-null     int64 \n",
      " 1   dialogue  28 non-null     object\n",
      " 2   summary   28 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 800.0+ bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42 entries, 0 to 41\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        42 non-null     int64 \n",
      " 1   dialogue  42 non-null     object\n",
      " 2   summary   42 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.1+ KB\n",
      "None \n",
      "\n",
      " None \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_0 = pd.read_csv(filepath_or_buffer=os.path.join(ami_root,'train.csv'))\n",
    "test_0= pd.read_csv(filepath_or_buffer=os.path.join(ami_root,'test.csv'))\n",
    "val_0 = pd.read_csv(filepath_or_buffer=os.path.join(ami_root,'validation.csv'))\n",
    "\n",
    "print(train_0.info(),'\\n\\n',test_0.info(),'\\n\\n',val_0.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogsum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12460 entries, 0 to 12459\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        12460 non-null  object\n",
      " 1   dialogue  12460 non-null  object\n",
      " 2   summary   12460 non-null  object\n",
      " 3   topic     12460 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 389.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        1500 non-null   object\n",
      " 1   dialogue  1500 non-null   object\n",
      " 2   summary   1500 non-null   object\n",
      " 3   topic     1500 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 47.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        500 non-null    object\n",
      " 1   dialogue  500 non-null    object\n",
      " 2   summary   500 non-null    object\n",
      " 3   topic     500 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 15.8+ KB\n",
      "None \n",
      "\n",
      " None \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_1 = pd.read_csv(filepath_or_buffer=os.path.join(dialogsum_root,'train.csv'))\n",
    "test_1= pd.read_csv(filepath_or_buffer=os.path.join(dialogsum_root,'test.csv'))\n",
    "val_1 = pd.read_csv(filepath_or_buffer=os.path.join(dialogsum_root,'validation.csv'))\n",
    "print(train_1.info(),'\\n\\n',test_1.info(),'\\n\\n',val_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samsum Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14732 entries, 0 to 14731\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        14732 non-null  object\n",
      " 1   summary   14732 non-null  object\n",
      " 2   dialogue  14732 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 345.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 819 entries, 0 to 818\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        819 non-null    object\n",
      " 1   summary   819 non-null    object\n",
      " 2   dialogue  819 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 19.3+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 818 entries, 0 to 817\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        818 non-null    object\n",
      " 1   summary   818 non-null    object\n",
      " 2   dialogue  818 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 19.3+ KB\n",
      "None \n",
      "\n",
      " None \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "train_2 = pd.read_json(path_or_buf=os.path.join(samsum_root,'train.json'))\n",
    "test_2= pd.read_json(path_or_buf=os.path.join(samsum_root,'test.json'))\n",
    "val_2 = pd.read_json(path_or_buf=os.path.join(samsum_root,'val.json'))\n",
    "print(train_2.info(),'\\n\\n',test_2.info(),'\\n\\n',val_2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 879 entries, 0 to 878\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   conversation_id            879 non-null    object\n",
      " 1   tweet_ids_sentence_offset  879 non-null    object\n",
      " 2   annotations                879 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 20.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110 entries, 0 to 109\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   conversation_id            110 non-null    object\n",
      " 1   tweet_ids_sentence_offset  110 non-null    object\n",
      " 2   annotations                110 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 110 entries, 0 to 109\n",
      "Data columns (total 3 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   conversation_id            110 non-null    object\n",
      " 1   tweet_ids_sentence_offset  110 non-null    object\n",
      " 2   annotations                110 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ KB\n",
      "None \n",
      "\n",
      " None \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "tweetsum_root = '.\\\\data\\\\TweetSum'\n",
    "train_3 = pd.read_json(path_or_buf=os.path.join(tweetsum_root,'train.jsonl'),lines=True)\n",
    "test_3= pd.read_json(path_or_buf=os.path.join(tweetsum_root,'test.jsonl'),lines=True)\n",
    "val_3 = pd.read_json(path_or_buf=os.path.join(tweetsum_root,'val.jsonl'),lines=True)\n",
    "print(train_3.info(),'\\n\\n',test_3.info(),'\\n\\n',val_3.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only relevant features (id, dialogue, summary)\n",
    "train_0 = train_0[['id', 'dialogue', 'summary']]\n",
    "val_0 = val_0[['id', 'dialogue', 'summary']]\n",
    "test_0 = test_0[['id', 'dialogue', 'summary']]\n",
    "\n",
    "train_1 = train_1[['id', 'dialogue', 'summary']]\n",
    "val_1 = val_1[['id', 'dialogue', 'summary']]\n",
    "test_1 = test_1[['id', 'dialogue', 'summary']]\n",
    "\n",
    "train_2 = train_2[['id', 'dialogue', 'summary']]\n",
    "val_2 = val_2[['id', 'dialogue', 'summary']]\n",
    "test_2 = test_2[['id', 'dialogue', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation sets into train_mix\n",
    "train_mix = pd.concat([train_0, val_0, train_1, val_1, train_2, val_2], ignore_index=True)\n",
    "train_mix = train_mix.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle the dataset\n",
    "\n",
    "# Split train_mix into train and validation (90/10 split)\n",
    "train_data, val_data = train_test_split(train_mix, test_size=0.1, random_state=42)\n",
    "\n",
    "# Combine all test sets into test_mix\n",
    "test_mix = pd.concat([test_0, test_1, test_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 25884\n",
      "Validation size: 2877\n",
      "Test size: 2347\n"
     ]
    }
   ],
   "source": [
    "# Print details for verification\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}\")\n",
    "print(f\"Test size: {len(test_mix)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the 'id' column to string for consistency\n",
    "train_data['id'] = train_data['id'].astype(str)\n",
    "val_data['id'] = val_data['id'].astype(str)\n",
    "test_mix['id'] = test_mix['id'].astype(str)\n",
    "\n",
    "# Ensure other columns are also strings for consistency\n",
    "train_data['dialogue'] = train_data['dialogue'].astype(str)\n",
    "val_data['dialogue'] = val_data['dialogue'].astype(str)\n",
    "test_mix['dialogue'] = test_mix['dialogue'].astype(str)\n",
    "\n",
    "train_data['summary'] = train_data['summary'].astype(str)\n",
    "val_data['summary'] = val_data['summary'].astype(str)\n",
    "test_mix['summary'] = test_mix['summary'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_mix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DatasetDict for compatibility\n",
    "train_dataset_dict = DatasetDict({\"train\": train_dataset})\n",
    "val_dataset_dict = DatasetDict({\"validation\": val_dataset})\n",
    "test_dataset_dict = DatasetDict({\"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfb9362f0cc4b51b133e005557079fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/25884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d2ee2993cc41dc8d8f6c1161bafc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3edd9c205dc46049b7b56ce1061b9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2347 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save datasets in Hugging Face format\n",
    "train_dataset_dict.save_to_disk(save_directory)\n",
    "val_dataset_dict.save_to_disk(save_directory)\n",
    "test_dataset_dict.save_to_disk(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
